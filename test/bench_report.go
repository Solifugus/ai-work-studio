package test

import (
	"fmt"
	"math"
	"os"
	"sort"
	"strings"
	"time"
)

// PerformanceThresholds define what constitutes good/poor performance for various metrics.
type PerformanceThresholds struct {
	// Latency thresholds (operations per second)
	StorageOpsPerSecGood       float64 // > this is good
	StorageOpsPerSecPoor       float64 // < this is concerning

	ManagerOpsPerSecGood       float64
	ManagerOpsPerSecPoor       float64

	CacheOpsPerSecGood         float64
	CacheOpsPerSecPoor         float64

	// Memory thresholds (MB per operation)
	MemoryPerOpGood            float64 // < this is good
	MemoryPerOpPoor            float64 // > this is concerning

	// P95 latency thresholds (milliseconds)
	P95LatencyGoodMs           float64 // < this is good
	P95LatencyPoorMs           float64 // > this is concerning
}

// DefaultPerformanceThresholds returns reasonable performance expectations.
func DefaultPerformanceThresholds() PerformanceThresholds {
	return PerformanceThresholds{
		StorageOpsPerSecGood: 1000,    // 1K ops/sec is good for storage
		StorageOpsPerSecPoor: 100,     // < 100 ops/sec is concerning

		ManagerOpsPerSecGood: 500,     // 500 ops/sec is good for managers
		ManagerOpsPerSecPoor: 50,      // < 50 ops/sec is concerning

		CacheOpsPerSecGood: 10000,     // 10K ops/sec is good for cache
		CacheOpsPerSecPoor: 1000,      // < 1K ops/sec is concerning

		MemoryPerOpGood: 1.0,          // < 1MB per op is good
		MemoryPerOpPoor: 10.0,         // > 10MB per op is concerning

		P95LatencyGoodMs: 10.0,        // < 10ms P95 is good
		P95LatencyPoorMs: 100.0,       // > 100ms P95 is concerning
	}
}

// PerformanceAnalysis contains analysis of benchmark results.
type PerformanceAnalysis struct {
	Category           string
	ResultsCount       int
	AvgOpsPerSec      float64
	AvgMemoryMB       float64
	AvgP95LatencyMs   float64

	// Performance rating
	OverallRating     string // "Excellent", "Good", "Fair", "Poor"
	OverallScore      float64 // 0-100

	// Issues and recommendations
	Issues            []string
	Recommendations   []string
	Bottlenecks       []string
}

// BenchmarkReport generates comprehensive performance analysis from benchmark results.
type BenchmarkReport struct {
	GeneratedAt    time.Time
	TotalBenchmarks int
	Categories     map[string]*PerformanceAnalysis
	Summary        string
	Recommendations []string
	RegressionAlerts []string
}

// GenerateReport creates a comprehensive markdown report from benchmark results.
func GenerateReport(suite *BenchmarkSuite, baselineFile string) (*BenchmarkReport, error) {
	report := &BenchmarkReport{
		GeneratedAt:    time.Now(),
		TotalBenchmarks: len(suite.Results),
		Categories:     make(map[string]*PerformanceAnalysis),
	}

	// Load baseline results for comparison
	var baseline *BenchmarkSuite
	if baselineFile != "" {
		// In a real implementation, we'd load baseline from file
		// For now, we'll generate synthetic baseline data
		baseline = generateSyntheticBaseline()
	}

	// Group results by category
	categoryResults := groupResultsByCategory(suite.Results)

	// Analyze each category
	thresholds := DefaultPerformanceThresholds()
	for category, results := range categoryResults {
		analysis := analyzeCategory(category, results, thresholds)
		report.Categories[category] = analysis
	}

	// Generate regression alerts if we have baseline
	if baseline != nil {
		report.RegressionAlerts = detectRegressions(suite, baseline)
	}

	// Generate overall summary and recommendations
	report.Summary = generateSummary(report.Categories)
	report.Recommendations = generateRecommendations(report.Categories)

	return report, nil
}

// groupResultsByCategory groups benchmark results by their category (storage, manager, cache, etc).
func groupResultsByCategory(results []BenchmarkResult) map[string][]BenchmarkResult {
	categories := make(map[string][]BenchmarkResult)

	for _, result := range results {
		category := categorizeResult(result.Name)
		categories[category] = append(categories[category], result)
	}

	return categories
}

// categorizeResult determines which category a benchmark belongs to.
func categorizeResult(name string) string {
	nameLower := strings.ToLower(name)

	if strings.Contains(nameLower, "storage") {
		return "Storage Layer"
	}
	if strings.Contains(nameLower, "manager") || strings.Contains(nameLower, "goal") || strings.Contains(nameLower, "method") || strings.Contains(nameLower, "objective") {
		return "Manager Layer"
	}
	if strings.Contains(nameLower, "cache") {
		return "Method Cache"
	}
	if strings.Contains(nameLower, "llm") || strings.Contains(nameLower, "router") || strings.Contains(nameLower, "budget") {
		return "LLM Layer"
	}
	if strings.Contains(nameLower, "workflow") || strings.Contains(nameLower, "integration") {
		return "Integration"
	}
	if strings.Contains(nameLower, "memory") || strings.Contains(nameLower, "concurrent") || strings.Contains(nameLower, "scenario") {
		return "Stress Testing"
	}

	return "Other"
}

// analyzeCategory performs detailed performance analysis for a category.
func analyzeCategory(category string, results []BenchmarkResult, thresholds PerformanceThresholds) *PerformanceAnalysis {
	if len(results) == 0 {
		return &PerformanceAnalysis{
			Category:     category,
			ResultsCount: 0,
			OverallRating: "No Data",
		}
	}

	analysis := &PerformanceAnalysis{
		Category:     category,
		ResultsCount: len(results),
	}

	// Calculate averages
	totalOpsPerSec := 0.0
	totalMemoryMB := 0.0
	totalP95Ms := 0.0

	for _, result := range results {
		totalOpsPerSec += result.OperationsPerSecond
		totalMemoryMB += result.AvgMemoryMB
		totalP95Ms += float64(result.P95.Nanoseconds()) / 1e6 // Convert to milliseconds
	}

	analysis.AvgOpsPerSec = totalOpsPerSec / float64(len(results))
	analysis.AvgMemoryMB = totalMemoryMB / float64(len(results))
	analysis.AvgP95LatencyMs = totalP95Ms / float64(len(results))

	// Determine performance thresholds for this category
	var goodOpsThreshold, poorOpsThreshold float64
	switch category {
	case "Storage Layer":
		goodOpsThreshold = thresholds.StorageOpsPerSecGood
		poorOpsThreshold = thresholds.StorageOpsPerSecPoor
	case "Manager Layer":
		goodOpsThreshold = thresholds.ManagerOpsPerSecGood
		poorOpsThreshold = thresholds.ManagerOpsPerSecPoor
	case "Method Cache":
		goodOpsThreshold = thresholds.CacheOpsPerSecGood
		poorOpsThreshold = thresholds.CacheOpsPerSecPoor
	default:
		goodOpsThreshold = thresholds.ManagerOpsPerSecGood
		poorOpsThreshold = thresholds.ManagerOpsPerSecPoor
	}

	// Calculate performance score (0-100)
	score := 0.0

	// Throughput score (40% weight)
	throughputScore := 0.0
	if analysis.AvgOpsPerSec >= goodOpsThreshold {
		throughputScore = 100.0
	} else if analysis.AvgOpsPerSec >= poorOpsThreshold {
		throughputScore = 50.0 + (analysis.AvgOpsPerSec-poorOpsThreshold)/(goodOpsThreshold-poorOpsThreshold)*50.0
	} else {
		throughputScore = math.Max(0, analysis.AvgOpsPerSec/poorOpsThreshold*50.0)
	}
	score += throughputScore * 0.4

	// Latency score (35% weight)
	latencyScore := 0.0
	if analysis.AvgP95LatencyMs <= thresholds.P95LatencyGoodMs {
		latencyScore = 100.0
	} else if analysis.AvgP95LatencyMs <= thresholds.P95LatencyPoorMs {
		latencyScore = 50.0 + (thresholds.P95LatencyPoorMs-analysis.AvgP95LatencyMs)/(thresholds.P95LatencyPoorMs-thresholds.P95LatencyGoodMs)*50.0
	} else {
		latencyScore = math.Max(0, 50.0-(analysis.AvgP95LatencyMs-thresholds.P95LatencyPoorMs)/10.0)
	}
	score += latencyScore * 0.35

	// Memory score (25% weight)
	memoryScore := 0.0
	if analysis.AvgMemoryMB <= thresholds.MemoryPerOpGood {
		memoryScore = 100.0
	} else if analysis.AvgMemoryMB <= thresholds.MemoryPerOpPoor {
		memoryScore = 50.0 + (thresholds.MemoryPerOpPoor-analysis.AvgMemoryMB)/(thresholds.MemoryPerOpPoor-thresholds.MemoryPerOpGood)*50.0
	} else {
		memoryScore = math.Max(0, 50.0-(analysis.AvgMemoryMB-thresholds.MemoryPerOpPoor))
	}
	score += memoryScore * 0.25

	analysis.OverallScore = score

	// Determine rating
	if score >= 85 {
		analysis.OverallRating = "Excellent"
	} else if score >= 70 {
		analysis.OverallRating = "Good"
	} else if score >= 50 {
		analysis.OverallRating = "Fair"
	} else {
		analysis.OverallRating = "Poor"
	}

	// Identify issues and recommendations
	analysis.Issues, analysis.Recommendations, analysis.Bottlenecks = identifyIssues(analysis, thresholds, results)

	return analysis
}

// identifyIssues analyzes performance data to find problems and suggest fixes.
func identifyIssues(analysis *PerformanceAnalysis, thresholds PerformanceThresholds, results []BenchmarkResult) ([]string, []string, []string) {
	var issues, recommendations, bottlenecks []string

	// Check throughput issues
	if analysis.AvgOpsPerSec < 100 {
		issues = append(issues, fmt.Sprintf("Low throughput: %.1f ops/sec", analysis.AvgOpsPerSec))
		recommendations = append(recommendations, "Consider optimizing critical path operations")
		bottlenecks = append(bottlenecks, "CPU-bound operations")
	}

	// Check latency issues
	if analysis.AvgP95LatencyMs > thresholds.P95LatencyPoorMs {
		issues = append(issues, fmt.Sprintf("High P95 latency: %.1fms", analysis.AvgP95LatencyMs))
		recommendations = append(recommendations, "Profile code to identify latency hotspots")
		bottlenecks = append(bottlenecks, "Slow operations")
	}

	// Check memory issues
	if analysis.AvgMemoryMB > thresholds.MemoryPerOpPoor {
		issues = append(issues, fmt.Sprintf("High memory usage: %.1fMB per operation", analysis.AvgMemoryMB))
		recommendations = append(recommendations, "Optimize memory allocations and object reuse")
		bottlenecks = append(bottlenecks, "Memory allocations")
	}

	// Check for high variance in results
	if len(results) > 1 {
		opsVariance := calculateVariance(results, func(r BenchmarkResult) float64 { return r.OperationsPerSecond })
		if opsVariance > analysis.AvgOpsPerSec*0.5 { // High variance
			issues = append(issues, "Inconsistent performance across runs")
			recommendations = append(recommendations, "Investigate performance variability causes")
		}
	}

	// Category-specific recommendations
	switch analysis.Category {
	case "Storage Layer":
		if analysis.AvgOpsPerSec < 500 {
			recommendations = append(recommendations, "Consider adding indexes or optimizing file I/O patterns")
		}
	case "Method Cache":
		if analysis.AvgOpsPerSec < 5000 {
			recommendations = append(recommendations, "Optimize cache lookup algorithms or increase cache size")
		}
	case "LLM Layer":
		if analysis.AvgP95LatencyMs > 50 {
			recommendations = append(recommendations, "Consider request batching or async processing")
		}
	}

	return issues, recommendations, bottlenecks
}

// calculateVariance calculates the variance of a metric across results.
func calculateVariance(results []BenchmarkResult, extractor func(BenchmarkResult) float64) float64 {
	if len(results) < 2 {
		return 0
	}

	// Calculate mean
	sum := 0.0
	for _, result := range results {
		sum += extractor(result)
	}
	mean := sum / float64(len(results))

	// Calculate variance
	variance := 0.0
	for _, result := range results {
		diff := extractor(result) - mean
		variance += diff * diff
	}
	variance /= float64(len(results) - 1)

	return variance
}

// detectRegressions compares current results with baseline to find performance regressions.
func detectRegressions(current, baseline *BenchmarkSuite) []string {
	var alerts []string

	baselineMap := make(map[string]BenchmarkResult)
	for _, result := range baseline.Results {
		baselineMap[result.Name] = result
	}

	for _, currentResult := range current.Results {
		if baselineResult, exists := baselineMap[currentResult.Name]; exists {
			// Check for significant performance degradation
			throughputRegression := (baselineResult.OperationsPerSecond - currentResult.OperationsPerSecond) / baselineResult.OperationsPerSecond
			latencyRegression := (float64(currentResult.P95.Nanoseconds()) - float64(baselineResult.P95.Nanoseconds())) / float64(baselineResult.P95.Nanoseconds())

			if throughputRegression > 0.2 { // 20% throughput drop
				alerts = append(alerts, fmt.Sprintf("âš ï¸ %s: Throughput dropped %.1f%% (%.1f â†’ %.1f ops/sec)",
					currentResult.Name, throughputRegression*100,
					baselineResult.OperationsPerSecond, currentResult.OperationsPerSecond))
			}

			if latencyRegression > 0.5 { // 50% latency increase
				alerts = append(alerts, fmt.Sprintf("âš ï¸ %s: P95 latency increased %.1f%% (%.1fms â†’ %.1fms)",
					currentResult.Name, latencyRegression*100,
					float64(baselineResult.P95.Nanoseconds())/1e6,
					float64(currentResult.P95.Nanoseconds())/1e6))
			}
		}
	}

	return alerts
}

// generateSummary creates an overall summary of performance.
func generateSummary(categories map[string]*PerformanceAnalysis) string {
	if len(categories) == 0 {
		return "No benchmark data available."
	}

	totalScore := 0.0
	excellentCount := 0
	goodCount := 0
	fairCount := 0
	poorCount := 0

	for _, analysis := range categories {
		totalScore += analysis.OverallScore
		switch analysis.OverallRating {
		case "Excellent":
			excellentCount++
		case "Good":
			goodCount++
		case "Fair":
			fairCount++
		case "Poor":
			poorCount++
		}
	}

	avgScore := totalScore / float64(len(categories))
	var overallRating string
	if avgScore >= 85 {
		overallRating = "Excellent"
	} else if avgScore >= 70 {
		overallRating = "Good"
	} else if avgScore >= 50 {
		overallRating = "Fair"
	} else {
		overallRating = "Poor"
	}

	return fmt.Sprintf("Overall Performance: **%s** (%.1f/100)\n\n"+
		"Category Breakdown:\n"+
		"- ðŸŸ¢ Excellent: %d categories\n"+
		"- ðŸ”µ Good: %d categories\n"+
		"- ðŸŸ¡ Fair: %d categories\n"+
		"- ðŸ”´ Poor: %d categories",
		overallRating, avgScore, excellentCount, goodCount, fairCount, poorCount)
}

// generateRecommendations creates prioritized recommendations.
func generateRecommendations(categories map[string]*PerformanceAnalysis) []string {
	var recommendations []string

	// Collect all recommendations with priority
	type prioritizedRec struct {
		text     string
		priority int
		score    float64
	}

	var allRecs []prioritizedRec

	for _, analysis := range categories {
		priority := 1
		if analysis.OverallRating == "Poor" {
			priority = 3
		} else if analysis.OverallRating == "Fair" {
			priority = 2
		}

		for _, rec := range analysis.Recommendations {
			allRecs = append(allRecs, prioritizedRec{
				text:     fmt.Sprintf("[%s] %s", analysis.Category, rec),
				priority: priority,
				score:    analysis.OverallScore,
			})
		}
	}

	// Sort by priority (high to low), then by score (low to high)
	sort.Slice(allRecs, func(i, j int) bool {
		if allRecs[i].priority != allRecs[j].priority {
			return allRecs[i].priority > allRecs[j].priority
		}
		return allRecs[i].score < allRecs[j].score
	})

	// Convert to strings and limit to top recommendations
	maxRecs := 10
	for i, rec := range allRecs {
		if i >= maxRecs {
			break
		}
		recommendations = append(recommendations, rec.text)
	}

	return recommendations
}

// WriteMarkdownReport writes the benchmark report as a markdown file.
func WriteMarkdownReport(report *BenchmarkReport, filename string) error {
	var md strings.Builder

	// Header
	md.WriteString("# AI Work Studio Performance Benchmark Report\n\n")
	md.WriteString(fmt.Sprintf("**Generated:** %s  \n", report.GeneratedAt.Format("2006-01-02 15:04:05 MST")))
	md.WriteString(fmt.Sprintf("**Total Benchmarks:** %d  \n", report.TotalBenchmarks))
	md.WriteString(fmt.Sprintf("**Test Duration:** %.2f seconds  \n\n", time.Since(report.GeneratedAt).Seconds()))

	// Summary
	md.WriteString("## Executive Summary\n\n")
	md.WriteString(report.Summary)
	md.WriteString("\n\n")

	// Regression Alerts
	if len(report.RegressionAlerts) > 0 {
		md.WriteString("## ðŸš¨ Performance Regression Alerts\n\n")
		for _, alert := range report.RegressionAlerts {
			md.WriteString(fmt.Sprintf("- %s\n", alert))
		}
		md.WriteString("\n")
	}

	// Category Analysis
	md.WriteString("## Detailed Performance Analysis\n\n")

	// Sort categories for consistent output
	var categoryNames []string
	for name := range report.Categories {
		categoryNames = append(categoryNames, name)
	}
	sort.Strings(categoryNames)

	for _, categoryName := range categoryNames {
		analysis := report.Categories[categoryName]

		md.WriteString(fmt.Sprintf("### %s\n\n", analysis.Category))

		// Performance summary table
		md.WriteString("| Metric | Value | Rating |\n")
		md.WriteString("|--------|-------|--------|\n")
		md.WriteString(fmt.Sprintf("| Overall Score | %.1f/100 | %s |\n", analysis.OverallScore, getRatingEmoji(analysis.OverallRating)))
		md.WriteString(fmt.Sprintf("| Avg Throughput | %.1f ops/sec | |\n", analysis.AvgOpsPerSec))
		md.WriteString(fmt.Sprintf("| Avg P95 Latency | %.1f ms | |\n", analysis.AvgP95LatencyMs))
		md.WriteString(fmt.Sprintf("| Avg Memory Usage | %.2f MB/op | |\n", analysis.AvgMemoryMB))
		md.WriteString(fmt.Sprintf("| Benchmarks Count | %d | |\n", analysis.ResultsCount))
		md.WriteString("\n")

		// Issues
		if len(analysis.Issues) > 0 {
			md.WriteString("**Issues Identified:**\n")
			for _, issue := range analysis.Issues {
				md.WriteString(fmt.Sprintf("- âš ï¸ %s\n", issue))
			}
			md.WriteString("\n")
		}

		// Bottlenecks
		if len(analysis.Bottlenecks) > 0 {
			md.WriteString("**Potential Bottlenecks:**\n")
			for _, bottleneck := range analysis.Bottlenecks {
				md.WriteString(fmt.Sprintf("- ðŸ” %s\n", bottleneck))
			}
			md.WriteString("\n")
		}

		// Recommendations
		if len(analysis.Recommendations) > 0 {
			md.WriteString("**Recommendations:**\n")
			for _, rec := range analysis.Recommendations {
				md.WriteString(fmt.Sprintf("- ðŸ’¡ %s\n", rec))
			}
			md.WriteString("\n")
		}

		md.WriteString("---\n\n")
	}

	// Top Recommendations
	if len(report.Recommendations) > 0 {
		md.WriteString("## ðŸŽ¯ Priority Recommendations\n\n")
		for i, rec := range report.Recommendations {
			md.WriteString(fmt.Sprintf("%d. %s\n", i+1, rec))
		}
		md.WriteString("\n")
	}

	// Technical Details
	md.WriteString("## Technical Specifications\n\n")
	md.WriteString("- **Go Version:** " + "1.22.2\n")
	md.WriteString("- **Platform:** Linux\n")
	md.WriteString("- **Benchmark Tool:** Go testing.B\n")
	md.WriteString("- **Memory Profiling:** Enabled\n")
	md.WriteString("- **Timing Method:** High-resolution monotonic clock\n\n")

	md.WriteString("## Benchmark Methodology\n\n")
	md.WriteString("- Each benchmark runs multiple iterations to achieve statistical significance\n")
	md.WriteString("- Memory measurements include both allocations and peak usage\n")
	md.WriteString("- Timing percentiles (P50, P95, P99) calculated from individual operation timings\n")
	md.WriteString("- Mock services used for LLM operations to ensure consistent timing\n")
	md.WriteString("- Isolated test environments prevent interference between benchmarks\n\n")

	md.WriteString("---\n")
	md.WriteString("*Report generated by AI Work Studio benchmark suite*\n")

	// Write to file
	return os.WriteFile(filename, []byte(md.String()), 0644)
}

// getRatingEmoji returns an emoji for the performance rating.
func getRatingEmoji(rating string) string {
	switch rating {
	case "Excellent":
		return "ðŸŸ¢ Excellent"
	case "Good":
		return "ðŸ”µ Good"
	case "Fair":
		return "ðŸŸ¡ Fair"
	case "Poor":
		return "ðŸ”´ Poor"
	default:
		return "âšª " + rating
	}
}

// generateSyntheticBaseline creates synthetic baseline data for regression testing.
func generateSyntheticBaseline() *BenchmarkSuite {
	// In a real implementation, this would load from a stored baseline file
	// For demonstration, we create slightly better baseline data
	baseline := &BenchmarkSuite{
		Started: time.Now().Add(-24 * time.Hour),
		Results: []BenchmarkResult{
			{
				Name:                "Storage_Node_Create",
				OperationsPerSecond: 1200,
				P95:                 8 * time.Millisecond,
				AvgMemoryMB:         0.8,
			},
			{
				Name:                "Storage_Node_Read",
				OperationsPerSecond: 15000,
				P95:                 2 * time.Millisecond,
				AvgMemoryMB:         0.1,
			},
			// Add more baseline results as needed
		},
	}
	return baseline
}

// CreateSampleReport creates a sample performance report for demonstration.
func CreateSampleReport() {
	// Create sample benchmark data
	suite := &BenchmarkSuite{
		Started: time.Now().Add(-10 * time.Minute),
		Results: []BenchmarkResult{
			{
				Name:                "Storage_Node_Create",
				Duration:            5 * time.Second,
				OperationsPerSecond: 1200,
				P50:                 5 * time.Millisecond,
				P95:                 8 * time.Millisecond,
				P99:                 15 * time.Millisecond,
				AvgMemoryMB:         0.8,
				Iterations:          1000,
			},
			{
				Name:                "MethodCache_Find",
				Duration:            2 * time.Second,
				OperationsPerSecond: 15000,
				P50:                 1 * time.Millisecond,
				P95:                 2 * time.Millisecond,
				P99:                 5 * time.Millisecond,
				AvgMemoryMB:         0.1,
				Iterations:          2000,
			},
			// Add more sample results as needed for demonstration
		},
	}

	report, err := GenerateReport(suite, "")
	if err != nil {
		fmt.Printf("Error generating report: %v\n", err)
		return
	}

	err = WriteMarkdownReport(report, "docs/performance_sample.md")
	if err != nil {
		fmt.Printf("Error writing report: %v\n", err)
		return
	}

	fmt.Printf("Sample performance report generated: docs/performance_sample.md\n")
	fmt.Printf("Overall Performance: %s\n", report.Summary)
}